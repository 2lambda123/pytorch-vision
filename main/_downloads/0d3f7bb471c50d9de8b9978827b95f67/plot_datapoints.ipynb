{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Datapoints FAQ\n\nThe :mod:`torchvision.datapoints` namespace was introduced together with ``torchvision.transforms.v2``. This example\nshowcases what these datapoints are and how they behave. This is a fairly low-level topic that most users will not need\nto worry about: you do not need to understand the internals of datapoints to efficiently rely on\n``torchvision.transforms.v2``. It may however be useful for advanced users trying to implement their own datasets,\ntransforms, or work directly with the datapoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import PIL.Image\n\nimport torch\nimport torchvision\n\n# We are using BETA APIs, so we deactivate the associated warning, thereby acknowledging that\n# some APIs may slightly change in the future\ntorchvision.disable_beta_transforms_warning()\n\nfrom torchvision import datapoints\nfrom torchvision.transforms.v2 import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What are datapoints?\n\nDatapoints are zero-copy tensor subclasses:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensor = torch.rand(3, 256, 256)\nimage = datapoints.Image(tensor)\n\nassert isinstance(image, torch.Tensor)\nassert image.data_ptr() == tensor.data_ptr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Under the hood, they are needed in :mod:`torchvision.transforms.v2` to correctly dispatch to the appropriate function\nfor the input data.\n\n## What datapoints are supported?\n\nSo far :mod:`torchvision.datapoints` supports four types of datapoints:\n\n* :class:`~torchvision.datapoints.Image`\n* :class:`~torchvision.datapoints.Video`\n* :class:`~torchvision.datapoints.BoundingBoxes`\n* :class:`~torchvision.datapoints.Mask`\n\n## How do I construct a datapoint?\n\nEach datapoint class takes any tensor-like data that can be turned into a :class:`~torch.Tensor`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image = datapoints.Image([[[[0, 1], [1, 0]]]])\nprint(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to other PyTorch creations ops, the constructor also takes the ``dtype``, ``device``, and ``requires_grad``\nparameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "float_image = datapoints.Image([[[0, 1], [1, 0]]], dtype=torch.float32, requires_grad=True)\nprint(float_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition, :class:`~torchvision.datapoints.Image` and :class:`~torchvision.datapoints.Mask` also take a\n:class:`PIL.Image.Image` directly:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image = datapoints.Image(PIL.Image.open(\"assets/astronaut.jpg\"))\nprint(image.shape, image.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In general, the datapoints can also store additional metadata that complements the underlying tensor. For example,\n:class:`~torchvision.datapoints.BoundingBoxes` stores the coordinate format as well as the spatial size of the\ncorresponding image alongside the actual values:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bounding_box = datapoints.BoundingBoxes(\n    [17, 16, 344, 495], format=datapoints.BoundingBoxFormat.XYXY, canvas_size=image.shape[-2:]\n)\nprint(bounding_box)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Do I have to wrap the output of the datasets myself?\n\nOnly if you are using custom datasets. For the built-in ones, you can use\n:func:`torchvision.datasets.wrap_dataset_for_transforms_v2`. Note that the function also supports subclasses of the\nbuilt-in datasets. Meaning, if your custom dataset subclasses from a built-in one and the output type is the same, you\nalso don't have to wrap manually.\n\nIf you have a custom dataset, for example the ``PennFudanDataset`` from\n[this tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html), you have two options:\n\n1. Perform the wrapping inside ``__getitem__``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PennFudanDataset(torch.utils.data.Dataset):\n    ...\n\n    def __getitem__(self, item):\n        ...\n\n        target[\"boxes\"] = datapoints.BoundingBoxes(\n            boxes,\n            format=datapoints.BoundingBoxFormat.XYXY,\n            canvas_size=F.get_size(img),\n        )\n        target[\"labels\"] = labels\n        target[\"masks\"] = datapoints.Mask(masks)\n\n        ...\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Perform the wrapping inside a custom transformation at the beginning of your pipeline:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class WrapPennFudanDataset:\n    def __call__(self, img, target):\n        target[\"boxes\"] = datapoints.BoundingBoxes(\n            target[\"boxes\"],\n            format=datapoints.BoundingBoxFormat.XYXY,\n            canvas_size=F.get_size(img),\n        )\n        target[\"masks\"] = datapoints.Mask(target[\"masks\"])\n        return img, target\n\n\n...\n\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(WrapPennFudanDataset())\n    transforms.append(T.PILToTensor())\n    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If both :class:`~torchvision.datapoints.BoundingBoxes`'es and :class:`~torchvision.datapoints.Mask`'s are included in\n   the sample, ``torchvision.transforms.v2`` will transform them both. Meaning, if you don't need both, dropping or\n   at least not wrapping the obsolete parts, can lead to a significant performance boost.\n\n   For example, if you are using the ``PennFudanDataset`` for object detection, not wrapping the masks avoids\n   transforming them over and over again in the pipeline just to ultimately ignoring them. In general, it would be\n   even better to not load the masks at all, but this is not possible in this example, since the bounding boxes are\n   generated from the masks.</p></div>\n\n## How do the datapoints behave inside a computation?\n\nDatapoints look and feel just like regular tensors. Everything that is supported on a plain :class:`torch.Tensor`\nalso works on datapoints.\nSince for most operations involving datapoints, it cannot be safely inferred whether the result should retain the\ndatapoint type, we choose to return a plain tensor instead of a datapoint (this might change, see note below):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert isinstance(image, datapoints.Image)\n\nnew_image = image + 0\n\nassert isinstance(new_image, torch.Tensor) and not isinstance(new_image, datapoints.Image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This \"unwrapping\" behaviour is something we're actively seeking feedback on. If you find this surprising or if you\n   have any suggestions on how to better support your use-cases, please reach out to us via this issue:\n   https://github.com/pytorch/vision/issues/7319</p></div>\n\nThere are two exceptions to this rule:\n\n1. The operations :meth:`~torch.Tensor.clone`, :meth:`~torch.Tensor.to`, and :meth:`~torch.Tensor.requires_grad_`\n   retain the datapoint type.\n2. Inplace operations on datapoints cannot change the type of the datapoint they are called on. However, if you use\n   the flow style, the returned value will be unwrapped:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image = datapoints.Image([[[0, 1], [1, 0]]])\n\nnew_image = image.add_(1).mul_(2)\n\nassert isinstance(image, torch.Tensor)\nprint(image)\n\nassert isinstance(new_image, torch.Tensor) and not isinstance(new_image, datapoints.Image)\nassert (new_image == image).all()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}