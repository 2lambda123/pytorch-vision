{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Datapoints FAQ\n\nDatapoints are Tensor subclasses introduced together with\n``torchvision.transforms.v2``. This example showcases what these datapoints are\nand how they behave.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>**Intended Audience** Unless you're writing your own transforms or your own datapoints, you\n    probably do not need to read this guide. This is a fairly low-level topic\n    that most users will not need to worry about: you do not need to understand\n    the internals of datapoints to efficiently rely on\n    ``torchvision.transforms.v2``. It may however be useful for advanced users\n    trying to implement their own datasets, transforms, or work directly with\n    the datapoints.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import PIL.Image\n\nimport torch\nimport torchvision\n\n# We are using BETA APIs, so we deactivate the associated warning, thereby acknowledging that\n# some APIs may slightly change in the future\ntorchvision.disable_beta_transforms_warning()\n\nfrom torchvision import datapoints\nfrom torchvision.transforms.v2 import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What are datapoints?\n\nDatapoints are zero-copy tensor subclasses:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensor = torch.rand(3, 256, 256)\nimage = datapoints.Image(tensor)\n\nassert isinstance(image, torch.Tensor)\nassert image.data_ptr() == tensor.data_ptr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Under the hood, they are needed in :mod:`torchvision.transforms.v2` to correctly dispatch to the appropriate function\nfor the input data.\n\n## What can I do with a datapoint?\n\nDatapoints look and feel just like regular tensors - they **are** tensors.\nEverything that is supported on a plain :class:`torch.Tensor` like ``.sum()`` or\nany ``torch.*`` operator will also works on datapoints. See\n`datapoint_unwrapping_behaviour` for a few gotchas.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What datapoints are supported?\n\nSo far :mod:`torchvision.datapoints` supports four types of datapoints:\n\n* :class:`~torchvision.datapoints.Image`\n* :class:`~torchvision.datapoints.Video`\n* :class:`~torchvision.datapoints.BoundingBoxes`\n* :class:`~torchvision.datapoints.Mask`\n\n\n## How do I construct a datapoint?\n\n### Using the constructor\n\nEach datapoint class takes any tensor-like data that can be turned into a :class:`~torch.Tensor`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image = datapoints.Image([[[[0, 1], [1, 0]]]])\nprint(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to other PyTorch creations ops, the constructor also takes the ``dtype``, ``device``, and ``requires_grad``\nparameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "float_image = datapoints.Image([[[0, 1], [1, 0]]], dtype=torch.float32, requires_grad=True)\nprint(float_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition, :class:`~torchvision.datapoints.Image` and :class:`~torchvision.datapoints.Mask` can also take a\n:class:`PIL.Image.Image` directly:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image = datapoints.Image(PIL.Image.open(\"assets/astronaut.jpg\"))\nprint(image.shape, image.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some datapoints require additional metadata to be passed in ordered to be constructed. For example,\n:class:`~torchvision.datapoints.BoundingBoxes` requires the coordinate format as well as the size of the\ncorresponding image (``canvas_size``) alongside the actual values. These\nmetadata are required to properly transform the bounding boxes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bboxes = datapoints.BoundingBoxes(\n    [[17, 16, 344, 495], [0, 10, 0, 10]],\n    format=datapoints.BoundingBoxFormat.XYXY,\n    canvas_size=image.shape[-2:]\n)\nprint(bboxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the ``wrap_like()`` class method\n\nYou can also use the ``wrap_like()`` class method to wrap a tensor object\ninto a datapoint. This is useful when you already have an object of the\ndesired type, which typically happens when writing transforms: you just want\nto wrap the output like the input. This API is inspired by utils like\n:func:`torch.zeros_like`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_bboxes = torch.tensor([0, 20, 30, 40])\nnew_bboxes = datapoints.BoundingBoxes.wrap_like(bboxes, new_bboxes)\nassert isinstance(new_bboxes, datapoints.BoundingBoxes)\nassert new_bboxes.canvas_size == bboxes.canvas_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The metadata of ``new_bboxes`` is the same as ``bboxes``, but you could pass\nit as a parameter to override it. Check the\n:meth:`~torchvision.datapoints.BoundingBoxes.wrap_like` documentation for\nmore details.\n\n## Do I have to wrap the output of the datasets myself?\n\nTODO: Move this in another guide - this is user-facing, not dev-facing.\n\nOnly if you are using custom datasets. For the built-in ones, you can use\n:func:`torchvision.datasets.wrap_dataset_for_transforms_v2`. Note that the function also supports subclasses of the\nbuilt-in datasets. Meaning, if your custom dataset subclasses from a built-in one and the output type is the same, you\nalso don't have to wrap manually.\n\nIf you have a custom dataset, for example the ``PennFudanDataset`` from\n[this tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html), you have two options:\n\n1. Perform the wrapping inside ``__getitem__``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PennFudanDataset(torch.utils.data.Dataset):\n    ...\n\n    def __getitem__(self, item):\n        ...\n\n        target[\"bboxes\"] = datapoints.BoundingBoxes(\n            bboxes,\n            format=datapoints.BoundingBoxFormat.XYXY,\n            canvas_size=F.get_size(img),\n        )\n        target[\"labels\"] = labels\n        target[\"masks\"] = datapoints.Mask(masks)\n\n        ...\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Perform the wrapping inside a custom transformation at the beginning of your pipeline:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class WrapPennFudanDataset:\n    def __call__(self, img, target):\n        target[\"boxes\"] = datapoints.BoundingBoxes(\n            target[\"boxes\"],\n            format=datapoints.BoundingBoxFormat.XYXY,\n            canvas_size=F.get_size(img),\n        )\n        target[\"masks\"] = datapoints.Mask(target[\"masks\"])\n        return img, target\n\n\n...\n\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(WrapPennFudanDataset())\n    transforms.append(T.PILToTensor())\n    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If both :class:`~torchvision.datapoints.BoundingBoxes` and :class:`~torchvision.datapoints.Mask`'s are included in\n   the sample, ``torchvision.transforms.v2`` will transform them both. Meaning, if you don't need both, dropping or\n   at least not wrapping the obsolete parts, can lead to a significant performance boost.\n\n   For example, if you are using the ``PennFudanDataset`` for object detection, not wrapping the masks avoids\n   transforming them over and over again in the pipeline just to ultimately ignoring them. In general, it would be\n   even better to not load the masks at all, but this is not possible in this example, since the bounding boxes are\n   generated from the masks.</p></div>\n\n\n## I had a Datapoint but now I have a Tensor. Help!\n\nFor a lot of operations involving datapoints, we cannot safely infer whether\nthe result should retain the datapoint type, so we choose to return a plain\ntensor instead of a datapoint (this might change, see note below):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert isinstance(bboxes, datapoints.BoundingBoxes)\n\n# Shift bboxes by 3 pixels in both H and W\nnew_bboxes = bboxes + 3\n\nassert isinstance(new_bboxes, torch.Tensor) and not isinstance(new_bboxes, datapoints.BoundingBoxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you're writing your own custom transforms or code involving datapoints, you\ncan re-wrap the output into a datapoint by just calling their constructor, or\nby using the ``.wrap_like()`` class method:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_bboxes = bboxes + 3\nnew_bboxes = datapoints.BoundingBoxes.wrap_like(bboxes, new_bboxes)\nassert isinstance(new_bboxes, datapoints.BoundingBoxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See more details above in `datapoint_creation`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You never need to re-wrap manually if you're using the built-in transforms\n   or their functional equivalents: this is automatically taken care of for\n   you.</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This \"unwrapping\" behaviour is something we're actively seeking feedback on. If you find this surprising or if you\n   have any suggestions on how to better support your use-cases, please reach out to us via this issue:\n   https://github.com/pytorch/vision/issues/7319</p></div>\n\nThere are a few exceptions to this \"unwrapping\" rule:\n\n1. Operations like :meth:`~torch.Tensor.clone`, :meth:`~torch.Tensor.to`,\n   :meth:`torch.Tensor.detach` and :meth:`~torch.Tensor.requires_grad_` retain\n   the datapoint type.\n2. Inplace operations on datapoints like ``.add_()`` preserve they type. However,\n   the **returned** value of inplace operations will be unwrapped into a pure\n   tensor:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image = datapoints.Image([[[0, 1], [1, 0]]])\n\nnew_image = image.add_(1).mul_(2)\n\n# image got transformed in-place and is still an Image datapoint, but new_image\n# is a Tensor. They share the same underlying data and they're equal, just\n# different classes.\nassert isinstance(image, datapoints.Image)\nprint(image)\n\nassert isinstance(new_image, torch.Tensor) and not isinstance(new_image, datapoints.Image)\nassert (new_image == image).all()\nassert new_image.data_ptr() == image.data_ptr()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}