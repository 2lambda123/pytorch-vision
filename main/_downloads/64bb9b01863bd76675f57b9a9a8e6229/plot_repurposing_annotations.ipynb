{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Repurposing annotations\n\nThe following example illustrates the operations available in the torchvision.ops module for repurposing object\nlocalization annotations for different tasks (e.g. transforming masks used by instance and panoptic segmentation\nmethods into bounding boxes used by object detection methods).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os.path\n\nimport PIL.Image\nimport matplotlib.patches\nimport matplotlib.pyplot\nimport numpy\nimport torch\nfrom torchvision.ops import masks_to_boxes\n\nASSETS_DIRECTORY = \"../test/assets\"\n\nmatplotlib.pyplot.rcParams[\"savefig.bbox\"] = \"tight\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Masks\nIn tasks like instance and panoptic segmentation, masks are commonly defined, and are defined by this package,\nas a multi-dimensional array (e.g. a NumPy array or a PyTorch tensor) with the following shape:\n\n      (objects, height, width)\n\nWhere objects is the number of annotated objects in the image. Each (height, width) object corresponds to exactly\none object. For example, if your input image has the dimensions 224 x 224 and has four annotated objects the shape\nof your masks annotation has the following shape:\n\n      (4, 224, 224).\n\nA nice property of masks is that they can be easily repurposed to be used in methods to solve a variety of object\nlocalization tasks.\n\n## Masks to bounding boxes\nFor example, the masks to bounding_boxes operation can be used to transform masks into bounding boxes that can be\nused in methods like Faster RCNN and YOLO.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with PIL.Image.open(os.path.join(ASSETS_DIRECTORY, \"masks.tiff\")) as image:\n    masks = torch.zeros((image.n_frames, image.height, image.width), dtype=torch.int)\n\n    for index in range(image.n_frames):\n        image.seek(index)\n\n        frame = numpy.array(image)\n\n        masks[index] = torch.tensor(frame)\n\nbounding_boxes = masks_to_boxes(masks)\n\nfigure = matplotlib.pyplot.figure()\n\na = figure.add_subplot(121)\nb = figure.add_subplot(122)\n\nlabeled_image = torch.sum(masks, 0)\n\na.imshow(labeled_image)\nb.imshow(labeled_image)\n\nfor bounding_box in bounding_boxes:\n    x0, y0, x1, y1 = bounding_box\n\n    rectangle = matplotlib.patches.Rectangle((x0, y0), x1 - x0, y1 - y0, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n\n    b.add_patch(rectangle)\n\na.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\nb.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}