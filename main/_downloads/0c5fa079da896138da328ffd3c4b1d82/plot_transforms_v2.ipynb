{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Getting started with transforms v2\n\nMost computer vision tasks are not supported out of the box by ``torchvision.transforms`` v1, since it only supports\nimages. ``torchvision.transforms.v2`` enables jointly transforming images, videos, bounding boxes, and masks. This\nexample showcases the core functionality of the new ``torchvision.transforms.v2`` API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pathlib\n\nimport torch\nimport torchvision\n\n\ndef load_data():\n    from torchvision.io import read_image\n    from torchvision import datapoints\n    from torchvision.ops import masks_to_boxes\n\n    assets_directory = pathlib.Path(\"assets\")\n\n    path = assets_directory / \"FudanPed00054.png\"\n    image = datapoints.Image(read_image(str(path)))\n    merged_masks = read_image(str(assets_directory / \"FudanPed00054_mask.png\"))\n\n    labels = torch.unique(merged_masks)[1:]\n\n    masks = datapoints.Mask(merged_masks == labels.view(-1, 1, 1))\n\n    bounding_boxes = datapoints.BoundingBox(\n        masks_to_boxes(masks), format=datapoints.BoundingBoxFormat.XYXY, spatial_size=image.shape[-2:]\n    )\n\n    return path, image, bounding_boxes, masks, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :mod:`torchvision.transforms.v2` API supports images, videos, bounding boxes, and instance and segmentation\nmasks. Thus, it offers native support for many Computer Vision tasks, like image and video classification, object\ndetection or instance and semantic segmentation. Still, the interface is the same, making\n:mod:`torchvision.transforms.v2` a drop-in replacement for the existing :mod:`torchvision.transforms` API, aka v1.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We are using BETA APIs, so we deactivate the associated warning, thereby acknowledging that\n# some APIs may slightly change in the future\ntorchvision.disable_beta_transforms_warning()\nimport torchvision.transforms.v2 as transforms\n\ntransform = transforms.Compose(\n    [\n        transforms.ColorJitter(contrast=0.5),\n        transforms.RandomRotation(30),\n        transforms.CenterCrop(480),\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":mod:`torchvision.transforms.v2` natively supports jointly transforming multiple inputs while making sure that\npotential random behavior is consistent across all inputs. However, it doesn't enforce a specific input structure or\norder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "path, image, bounding_boxes, masks, labels = load_data()\n\ntorch.manual_seed(0)\nnew_image = transform(image)  # Image Classification\nnew_image, new_bounding_boxes, new_labels = transform(image, bounding_boxes, labels)  # Object Detection\nnew_image, new_bounding_boxes, new_masks, new_labels = transform(\n    image, bounding_boxes, masks, labels\n)  # Instance Segmentation\nnew_image, new_target = transform((image, {\"boxes\": bounding_boxes, \"labels\": labels}))  # Arbitrary Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Under the hood, :mod:`torchvision.transforms.v2` relies on :mod:`torchvision.datapoints` for the dispatch to the\nappropriate function for the input data: `sphx_glr_auto_examples_plot_datapoints.py`. Note however, that as\nregular user, you likely don't have to touch this yourself. See\n`sphx_glr_auto_examples_plot_transforms_v2_e2e.py`.\n\nAll \"foreign\" types like :class:`str`'s or :class:`pathlib.Path`'s are passed through, allowing to store extra\ninformation directly with the sample:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample = {\"path\": path, \"image\": image}\nnew_sample = transform(sample)\n\nassert new_sample[\"path\"] is sample[\"path\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As stated above, :mod:`torchvision.transforms.v2` is a drop-in replacement for :mod:`torchvision.transforms` and thus\nalso supports transforming plain :class:`torch.Tensor`'s as image or video if applicable. This is achieved with a\nsimple heuristic:\n\n* If we find an explicit image or video (:class:`torchvision.datapoints.Image`, :class:`torchvision.datapoints.Video`,\n  or :class:`PIL.Image.Image`) in the input, all other plain tensors are passed through.\n* If there is no explicit image or video, only the first plain :class:`torch.Tensor` will be transformed as image or\n  video, while all others will be passed through.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plain_tensor_image = torch.rand(image.shape)\n\nprint(image.shape, plain_tensor_image.shape)\n\n# passing a plain tensor together with an explicit image, will not transform the former\nplain_tensor_image, image = transform(plain_tensor_image, image)\n\nprint(image.shape, plain_tensor_image.shape)\n\n# passing a plain tensor without an explicit image, will transform the former\nplain_tensor_image, _ = transform(plain_tensor_image, bounding_boxes)\n\nprint(image.shape, plain_tensor_image.shape)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}