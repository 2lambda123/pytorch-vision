{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Datapoints FAQ\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Try on [collab](https://colab.research.google.com/github/pytorch/vision/blob/gh-pages/main/_generated_ipynb_notebooks/plot_datapoints.ipynb)\n    or `go to the end <sphx_glr_download_auto_examples_v2_transforms_plot_datapoints.py>` to download the full example code.</p></div>\n\n\nDatapoints are Tensor subclasses introduced together with\n``torchvision.transforms.v2``. This example showcases what these datapoints are\nand how they behave.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>**Intended Audience** Unless you're writing your own transforms or your own datapoints, you\n    probably do not need to read this guide. This is a fairly low-level topic\n    that most users will not need to worry about: you do not need to understand\n    the internals of datapoints to efficiently rely on\n    ``torchvision.transforms.v2``. It may however be useful for advanced users\n    trying to implement their own datasets, transforms, or work directly with\n    the datapoints.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import PIL.Image\n\nimport torch\nfrom torchvision import datapoints\nfrom torchvision.transforms.v2 import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What are datapoints?\n\nDatapoints are zero-copy tensor subclasses:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensor = torch.rand(3, 256, 256)\nimage = datapoints.Image(tensor)\n\nassert isinstance(image, torch.Tensor)\nassert image.data_ptr() == tensor.data_ptr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Under the hood, they are needed in :mod:`torchvision.transforms.v2` to correctly dispatch to the appropriate function\nfor the input data.\n\n:mod:`torchvision.datapoints` supports four types of datapoints:\n\n* :class:`~torchvision.datapoints.Image`\n* :class:`~torchvision.datapoints.Video`\n* :class:`~torchvision.datapoints.BoundingBoxes`\n* :class:`~torchvision.datapoints.Mask`\n\n## What can I do with a datapoint?\n\nDatapoints look and feel just like regular tensors - they **are** tensors.\nEverything that is supported on a plain :class:`torch.Tensor` like ``.sum()`` or\nany ``torch.*`` operator will also work on datapoints. See\n`datapoint_unwrapping_behaviour` for a few gotchas.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## How do I construct a datapoint?\n\n### Using the constructor\n\nEach datapoint class takes any tensor-like data that can be turned into a :class:`~torch.Tensor`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image = datapoints.Image([[[[0, 1], [1, 0]]]])\nprint(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to other PyTorch creations ops, the constructor also takes the ``dtype``, ``device``, and ``requires_grad``\nparameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "float_image = datapoints.Image([[[0, 1], [1, 0]]], dtype=torch.float32, requires_grad=True)\nprint(float_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition, :class:`~torchvision.datapoints.Image` and :class:`~torchvision.datapoints.Mask` can also take a\n:class:`PIL.Image.Image` directly:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image = datapoints.Image(PIL.Image.open(\"../assets/astronaut.jpg\"))\nprint(image.shape, image.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some datapoints require additional metadata to be passed in ordered to be constructed. For example,\n:class:`~torchvision.datapoints.BoundingBoxes` requires the coordinate format as well as the size of the\ncorresponding image (``canvas_size``) alongside the actual values. These\nmetadata are required to properly transform the bounding boxes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bboxes = datapoints.BoundingBoxes(\n    [[17, 16, 344, 495], [0, 10, 0, 10]],\n    format=datapoints.BoundingBoxFormat.XYXY,\n    canvas_size=image.shape[-2:]\n)\nprint(bboxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using ``datapoints.wrap()``\n\nYou can also use the :func:`~torchvision.datapoints.wrap` function to wrap a tensor object\ninto a datapoint. This is useful when you already have an object of the\ndesired type, which typically happens when writing transforms: you just want\nto wrap the output like the input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_bboxes = torch.tensor([0, 20, 30, 40])\nnew_bboxes = datapoints.wrap(new_bboxes, like=bboxes)\nassert isinstance(new_bboxes, datapoints.BoundingBoxes)\nassert new_bboxes.canvas_size == bboxes.canvas_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The metadata of ``new_bboxes`` is the same as ``bboxes``, but you could pass\nit as a parameter to override it.\n\n## Do I have to wrap the output of the datasets myself?\n\nTODO: Move this in another guide - this is user-facing, not dev-facing.\n\nOnly if you are using custom datasets. For the built-in ones, you can use\n:func:`torchvision.datasets.wrap_dataset_for_transforms_v2`. Note that the function also supports subclasses of the\nbuilt-in datasets. Meaning, if your custom dataset subclasses from a built-in one and the output type is the same, you\nalso don't have to wrap manually.\n\nIf you have a custom dataset, for example the ``PennFudanDataset`` from\n[this tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html), you have two options:\n\n1. Perform the wrapping inside ``__getitem__``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PennFudanDataset(torch.utils.data.Dataset):\n    ...\n\n    def __getitem__(self, item):\n        ...\n\n        target[\"bboxes\"] = datapoints.BoundingBoxes(\n            bboxes,\n            format=datapoints.BoundingBoxFormat.XYXY,\n            canvas_size=F.get_size(img),\n        )\n        target[\"labels\"] = labels\n        target[\"masks\"] = datapoints.Mask(masks)\n\n        ...\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Perform the wrapping inside a custom transformation at the beginning of your pipeline:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class WrapPennFudanDataset:\n    def __call__(self, img, target):\n        target[\"boxes\"] = datapoints.BoundingBoxes(\n            target[\"boxes\"],\n            format=datapoints.BoundingBoxFormat.XYXY,\n            canvas_size=F.get_size(img),\n        )\n        target[\"masks\"] = datapoints.Mask(target[\"masks\"])\n        return img, target\n\n\n...\n\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(WrapPennFudanDataset())\n    transforms.append(T.PILToTensor())\n    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If both :class:`~torchvision.datapoints.BoundingBoxes` and :class:`~torchvision.datapoints.Mask`'s are included in\n   the sample, ``torchvision.transforms.v2`` will transform them both. Meaning, if you don't need both, dropping or\n   at least not wrapping the obsolete parts, can lead to a significant performance boost.\n\n   For example, if you are using the ``PennFudanDataset`` for object detection, not wrapping the masks avoids\n   transforming them over and over again in the pipeline just to ultimately ignoring them. In general, it would be\n   even better to not load the masks at all, but this is not possible in this example, since the bounding boxes are\n   generated from the masks.</p></div>\n\n\n## I had a Datapoint but now I have a Tensor. Help!\n\nBy default, operations on :class:`~torchvision.datapoints.Datapoint` objects\nwill return a pure Tensor:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert isinstance(bboxes, datapoints.BoundingBoxes)\n\n# Shift bboxes by 3 pixels in both H and W\nnew_bboxes = bboxes + 3\n\nassert isinstance(new_bboxes, torch.Tensor)\nassert not isinstance(new_bboxes, datapoints.BoundingBoxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This behavior only affects native ``torch`` operations. If you are using\n   the built-in ``torchvision`` transforms or functionals, you will always get\n   as output the same type that you passed as input (pure ``Tensor`` or\n   ``Datapoint``).</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### But I want a Datapoint back!\n\nYou can re-wrap a pure tensor into a datapoint by just calling the datapoint\nconstructor, or by using the :func:`~torchvision.datapoints.wrap` function\n(see more details above in `datapoint_creation`):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_bboxes = bboxes + 3\nnew_bboxes = datapoints.wrap(new_bboxes, like=bboxes)\nassert isinstance(new_bboxes, datapoints.BoundingBoxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, you can use the :func:`~torchvision.datapoints.set_return_type`\nas a global config setting for the whole program, or as a context manager:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with datapoints.set_return_type(\"datapoint\"):\n    new_bboxes = bboxes + 3\nassert isinstance(new_bboxes, datapoints.BoundingBoxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why is this happening?\n\n**For performance reasons**. :class:`~torchvision.datapoints.Datapoint`\nclasses are Tensor subclasses, so any operation involving a\n:class:`~torchvision.datapoints.Datapoint` object will go through the\n[__torch_function__](https://pytorch.org/docs/stable/notes/extending.html#extending-torch)\nprotocol. This induces a small overhead, which we want to avoid when possible.\nThis doesn't matter for built-in ``torchvision`` transforms because we can\navoid the overhead there, but it could be a problem in your model's\n``forward``.\n\n**The alternative isn't much better anyway.** For every operation where\npreserving the :class:`~torchvision.datapoints.Datapoint` type makes\nsense, there are just as many operations where returning a pure Tensor is\npreferable: for example, is ``img.sum()`` still an :class:`~torchvision.datapoints.Image`?\nIf we were to preserve :class:`~torchvision.datapoints.Datapoint` types all\nthe way, even model's logits or the output of the loss function would end up\nbeing of type :class:`~torchvision.datapoints.Image`, and surely that's not\ndesirable.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This behaviour is something we're actively seeking feedback on. If you find this surprising or if you\n   have any suggestions on how to better support your use-cases, please reach out to us via this issue:\n   https://github.com/pytorch/vision/issues/7319</p></div>\n\n### Exceptions\n\nThere are a few exceptions to this \"unwrapping\" rule:\n\n1. Operations like :meth:`~torch.Tensor.clone`, :meth:`~torch.Tensor.to`,\n   :meth:`torch.Tensor.detach` and :meth:`~torch.Tensor.requires_grad_` retain\n   the datapoint type.\n2. Inplace operations on datapoints like ``.add_()`` preserve they type. However,\n   the **returned** value of inplace operations will be unwrapped into a pure\n   tensor:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image = datapoints.Image([[[0, 1], [1, 0]]])\n\nnew_image = image.add_(1).mul_(2)\n\n# image got transformed in-place and is still an Image datapoint, but new_image\n# is a Tensor. They share the same underlying data and they're equal, just\n# different classes.\nassert isinstance(image, datapoints.Image)\nprint(image)\n\nassert isinstance(new_image, torch.Tensor) and not isinstance(new_image, datapoints.Image)\nassert (new_image == image).all()\nassert new_image.data_ptr() == image.data_ptr()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}